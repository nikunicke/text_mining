{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 1 - CHAPTER 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import brown\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver as wd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyph_splitter(sample):\n",
    "    return sample[:(sample.index('-'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish\n",
      "run\n",
      "nation\n",
      "un\n",
      "pre\n"
     ]
    }
   ],
   "source": [
    "print(hyph_splitter('dish-es'))\n",
    "print(hyph_splitter('run-ning'))\n",
    "print(hyph_splitter('nation-nality'))\n",
    "print(hyph_splitter('un-do'))\n",
    "print(hyph_splitter('pre-heat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pto\n",
      "otP\n",
      "MtP\n",
      "PtM\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "monty = 'Monty Python'\n",
    "print(monty[6:11:2])\n",
    "print(monty[10:5:-2])\n",
    "print(monty[0:7:3])\n",
    "print(monty[6::-3])\n",
    "print(monty[::-10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 6\n",
    "\n",
    "    [a-zA-Z]+\n",
    "    \n",
    "a. Matches all consective alphabetical characters. No numbers or special characters.\n",
    "\n",
    "    [A-Z][a-z]*\n",
    "    \n",
    "b. Matches any capital letter and any consecutive letters. No numbers or special characters.\n",
    "\n",
    "    p[aeiou]{,2}t\n",
    "    \n",
    "c. Matches a string starting with 'p' followed by no more than two of 'a', 'e', 'i', 'o' or 'u', and ending with 't'. I.E. 'put', 'pout' but not 'pouat'.\n",
    "\n",
    "    \\d+(\\.\\d+)?\n",
    "    \n",
    "d. Matches between one and unlimited times. /d matches digits and + matches all the following digits. \\. matches . and any amount of following numbers. This regex matches decimal numbers.\n",
    "\n",
    "    ([^aeiou][aeiou][^aeiou])*\n",
    "    \n",
    "e. Matches between zore to unlimited times. [^aeiou] matches anything but the characters inside the brackets. [aeiou] only matches a character inide the brackets. This regex matches i.e. Bob.\n",
    "\n",
    "    \\w+|[^\\w\\s]+\n",
    "    \n",
    "f. Matches two alternatives. First alternative matches any letter or number and all the following letters and numbers. The second alternative matches all special characters exluding space/whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex:  [a-zA-Z]+\n",
      "Result:\n",
      " ['Bob', 'bought', 'chocolate', 'bars', 'for', 'and', 'put', 'them', 'in', 'his', 'candy', 'box'] \n",
      "\n",
      "Regex:  [A-Z][a-z]*\n",
      "Result:\n",
      " ['Bob'] \n",
      "\n",
      "Regex:  p[aeiou]{,2}t\n",
      "Result:\n",
      " ['put'] \n",
      "\n",
      "Regex:  \\d+(\\.\\d+)?\n",
      "Result:\n",
      " ['', '.35'] \n",
      "\n",
      "Regex:  ([^aeiou][aeiou][^aeiou])*\n",
      "Result:\n",
      " ['Bob', '', '', '', '', '', '', '', '', '', '', '', '', 'hoc', '', 'lat', '', '', 'bar', '', '', 'for', '', '', '', '', '', '', '', ' an', '', '', 'put', '', '', ' in', '', 'his', '', 'can', '', '', '', 'box', ''] \n",
      "\n",
      "Regex:  \\w+|[^\\w\\s]+\n",
      "Result:\n",
      " ['Bob', 'bought', '10', 'chocolate', 'bars', 'for', '$', '10', '.', '35', 'and', 'put', 'them', 'in', 'his', 'candy', 'box'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentance = 'Bob bought 10 chocolate bars for $10.35 and put them in his candy box'\n",
    "\n",
    "reg_list = [('[a-zA-Z]+'), ('[A-Z][a-z]*'), ('p[aeiou]{,2}t'), ('\\d+(\\.\\d+)?'), ('([^aeiou][aeiou][^aeiou])*'), ('\\w+|[^\\w\\s]+')]\n",
    "\n",
    "for reg in reg_list:\n",
    "    print('Regex: ', reg)\n",
    "    print('Result:\\n', re.findall((reg), sentance), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 8\n",
    "Input the url to an article from nytimes.com\n",
    "\n",
    "Usually formatted as following:\n",
    "\n",
    "    https://www.nytimes.com/year/month/day/category/title-of-article....\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def times_article_paragraph(url):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "\n",
    "    raw = bs(html, 'html.parser')\n",
    "    article = raw.find('div', attrs={'class': 'css-53u6y8'})\n",
    "#     print(article)\n",
    "    paragraph_list = list(article.get_text().split())\n",
    "    \n",
    "    paragraph = nltk.Text(paragraph_list)\n",
    "\n",
    "    print(paragraph, '\\n')\n",
    "    print(paragraph.concordance('Trump'))\n",
    "    \n",
    "    tokens = paragraph.tokens\n",
    "    \n",
    "    \n",
    "    return tokens\n",
    "#     print(paragraph.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: ALEXANDRIA, Va. — Paul Manafort, the political consultant...> \n",
      "\n",
      "Displaying 2 of 2 matches:\n",
      "nafort, the political consultant and Trump presidential campaign chairman whose\n",
      "d statement.Of the half-dozen former Trump associates prosecuted by Mr. Mueller\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ALEXANDRIA,',\n",
       " 'Va.',\n",
       " '—',\n",
       " 'Paul',\n",
       " 'Manafort,',\n",
       " 'the',\n",
       " 'political',\n",
       " 'consultant',\n",
       " 'and',\n",
       " 'Trump',\n",
       " 'presidential',\n",
       " 'campaign',\n",
       " 'chairman',\n",
       " 'whose',\n",
       " 'lucrative',\n",
       " 'work',\n",
       " 'in',\n",
       " 'Ukraine',\n",
       " 'and',\n",
       " 'ties',\n",
       " 'to',\n",
       " 'well-connected',\n",
       " 'Russians',\n",
       " 'made',\n",
       " 'him',\n",
       " 'a',\n",
       " 'target',\n",
       " 'of',\n",
       " 'the',\n",
       " 'special',\n",
       " 'counsel,',\n",
       " 'Robert',\n",
       " 'S.',\n",
       " 'Mueller',\n",
       " 'III,',\n",
       " 'was',\n",
       " 'sentenced',\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'to',\n",
       " 'nearly',\n",
       " 'four',\n",
       " 'years',\n",
       " 'in',\n",
       " 'prison',\n",
       " 'in',\n",
       " 'the',\n",
       " 'financial',\n",
       " 'fraud',\n",
       " 'case',\n",
       " 'that',\n",
       " 'left',\n",
       " 'his',\n",
       " 'grand',\n",
       " 'lifestyle',\n",
       " 'and',\n",
       " 'power-broker',\n",
       " 'reputation',\n",
       " 'in',\n",
       " 'ruins.The',\n",
       " 'sentence',\n",
       " 'in',\n",
       " 'the',\n",
       " 'highest-profile',\n",
       " 'criminal',\n",
       " 'case',\n",
       " 'mounted',\n",
       " 'by',\n",
       " 'the',\n",
       " 'special',\n",
       " 'counsel’s',\n",
       " 'office',\n",
       " 'was',\n",
       " 'far',\n",
       " 'lighter',\n",
       " 'than',\n",
       " 'the',\n",
       " '19-',\n",
       " 'to',\n",
       " '24-year',\n",
       " 'prison',\n",
       " 'term',\n",
       " 'recommended',\n",
       " 'under',\n",
       " 'sentencing',\n",
       " 'guidelines.',\n",
       " 'Judge',\n",
       " 'T.',\n",
       " 'S.',\n",
       " 'Ellis',\n",
       " 'III',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " 'District',\n",
       " 'Court',\n",
       " 'in',\n",
       " 'Alexandria,',\n",
       " 'Va.,',\n",
       " 'said',\n",
       " 'that',\n",
       " 'although',\n",
       " 'Mr.',\n",
       " 'Manafort’s',\n",
       " 'crimes',\n",
       " 'were',\n",
       " '“very',\n",
       " 'serious,”',\n",
       " 'following',\n",
       " 'the',\n",
       " 'guidelines',\n",
       " 'would',\n",
       " 'have',\n",
       " 'resulted',\n",
       " 'in',\n",
       " 'an',\n",
       " 'unduly',\n",
       " 'harsh',\n",
       " 'punishment.A',\n",
       " 'team',\n",
       " 'of',\n",
       " 'Mr.',\n",
       " 'Mueller’s',\n",
       " 'prosecutors',\n",
       " 'sat',\n",
       " 'glum-faced',\n",
       " 'as',\n",
       " 'Judge',\n",
       " 'Ellis',\n",
       " 'delivered',\n",
       " 'his',\n",
       " 'decision.',\n",
       " 'Mr.',\n",
       " 'Manafort,',\n",
       " 'who',\n",
       " 'has',\n",
       " 'gout',\n",
       " 'and',\n",
       " 'came',\n",
       " 'to',\n",
       " 'the',\n",
       " 'hearing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'wheelchair',\n",
       " 'with',\n",
       " 'his',\n",
       " 'foot',\n",
       " 'heavily',\n",
       " 'bandaged,',\n",
       " 'had',\n",
       " 'asked',\n",
       " 'the',\n",
       " 'judge',\n",
       " 'for',\n",
       " 'compassion.',\n",
       " '“To',\n",
       " 'say',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'humiliated',\n",
       " 'and',\n",
       " 'ashamed',\n",
       " 'would',\n",
       " 'be',\n",
       " 'a',\n",
       " 'gross',\n",
       " 'understatement,”',\n",
       " 'he',\n",
       " 'said',\n",
       " 'in',\n",
       " 'a',\n",
       " 'barely',\n",
       " 'audible',\n",
       " 'voice,',\n",
       " 'reading',\n",
       " 'from',\n",
       " 'a',\n",
       " 'prepared',\n",
       " 'statement.Of',\n",
       " 'the',\n",
       " 'half-dozen',\n",
       " 'former',\n",
       " 'Trump',\n",
       " 'associates',\n",
       " 'prosecuted',\n",
       " 'by',\n",
       " 'Mr.',\n",
       " 'Mueller,',\n",
       " 'Mr.',\n",
       " 'Manafort',\n",
       " 'garnered',\n",
       " 'the',\n",
       " 'harshest',\n",
       " 'punishment',\n",
       " 'yet',\n",
       " 'in',\n",
       " 'the',\n",
       " 'case',\n",
       " 'that',\n",
       " 'came',\n",
       " 'to',\n",
       " 'a',\n",
       " 'conclusion',\n",
       " 'on',\n",
       " 'Thursday',\n",
       " '—',\n",
       " 'the',\n",
       " 'first',\n",
       " 'of',\n",
       " 'two',\n",
       " 'for',\n",
       " 'which',\n",
       " 'Mr.',\n",
       " 'Manafort',\n",
       " 'is',\n",
       " 'being',\n",
       " 'sentenced',\n",
       " 'this',\n",
       " 'month.',\n",
       " 'While',\n",
       " 'prosecutors',\n",
       " 'sought',\n",
       " 'no',\n",
       " 'specific',\n",
       " 'sentence,',\n",
       " 'some',\n",
       " 'legal',\n",
       " 'experts',\n",
       " 'said',\n",
       " 'a',\n",
       " 'prison',\n",
       " 'term',\n",
       " 'that',\n",
       " 'amounts',\n",
       " 'to',\n",
       " 'one-fifth',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lightest',\n",
       " 'punishment',\n",
       " 'recommended',\n",
       " 'had',\n",
       " 'to',\n",
       " 'disappoint',\n",
       " 'them.']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_article_paragraph('https://www.nytimes.com/2019/03/07/us/politics/paul-manafort-sentencing.html?action=click&module=Top%20Stories&pgtype=Homepage')\n",
    "\n",
    "# 'https://www.nytimes.com/2019/03/07/us/politics/paul-manafort-sentencing.html?action=click&module=Top%20Stories&pgtype=Homepage'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars(string):\n",
    "    for char in string:\n",
    "        print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "s\n",
      "c\n",
      "a\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "chars('oscar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prog import monty\n",
    "monty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 18\n",
    "No duplicates in the frequency distrubution list for brown.words(categories = 'news'. If other texts causes problems because of case distinctions or punctuation you could try fixing it by modifying the regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wh_word_types(text):\n",
    "    wh_words = []\n",
    "    for word in text:\n",
    "        if re.match('wh', word) is not None:\n",
    "            wh_words.append(word)\n",
    "\n",
    "    fdist = nltk.FreqDist(w for w in wh_words)\n",
    "    return fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('who', 268),\n",
       " ('which', 244),\n",
       " ('when', 128),\n",
       " ('what', 76),\n",
       " ('where', 58),\n",
       " ('while', 43),\n",
       " ('whose', 22),\n",
       " ('whether', 18),\n",
       " ('white', 17),\n",
       " ('whole', 11),\n",
       " ('why', 9),\n",
       " ('whom', 8),\n",
       " ('whereby', 3),\n",
       " ('wheel', 3),\n",
       " ('whipped', 2),\n",
       " ('whiz', 2),\n",
       " ('whip', 2),\n",
       " ('wheeled', 2),\n",
       " ('whites', 2),\n",
       " ('wherever', 1),\n",
       " ('wholesale', 1),\n",
       " ('whatever', 1),\n",
       " ('whirling', 1),\n",
       " ('whisking', 1),\n",
       " ('wheels', 1),\n",
       " ('whopping', 1),\n",
       " ('wholly-owned', 1),\n",
       " ('whims', 1),\n",
       " ('white-clad', 1),\n",
       " ('wheat', 1),\n",
       " ('whiplash', 1),\n",
       " ('whichever', 1),\n",
       " ('wholly', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_word_types(brown.words(categories = 'news'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 19\n",
    "If you want to try with your own text file, make sure that the words and numbers are sorted as following:\n",
    "\n",
    "word1 number\n",
    "word2 number\n",
    "word3 number\n",
    "\n",
    "etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_list(textfile):\n",
    "    text_file = open(textfile).readlines()\n",
    "    text_file = re.sub('[\\s]', ',', re.sub('\\\\n', '', (' '.join(text_file)))).split(',')\n",
    "\n",
    "    freqList = []\n",
    "\n",
    "    print('Length of word list: ', len(text_file))\n",
    "\n",
    "    for i in range(0, len(text_file), 2):\n",
    "        freqList.append([text_file[i], int(text_file[i + 1])])\n",
    "\n",
    "    return freqList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of word list:  38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['and', 12],\n",
       " ['Niklas', 2],\n",
       " ['Walter', 3],\n",
       " ['Arcada', 49],\n",
       " ['plant', 123],\n",
       " ['water', 23],\n",
       " ['pen', 19],\n",
       " ['notebook', 98],\n",
       " ['laptop', 55],\n",
       " ['cable', 77],\n",
       " ['calendar', 33],\n",
       " ['flyer', 11],\n",
       " ['phone', 44],\n",
       " ['building', 89],\n",
       " ['floor', 234],\n",
       " ['couch', 1],\n",
       " ['ceiling', 345],\n",
       " ['wall', 2422],\n",
       " ['carpet', 1341]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list('words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 20\n",
    "Extracting data of how much time until the next Formula One race and where it will take place.\n",
    "\n",
    "Location/Days/Hours/Minutes\n",
    "\n",
    "Used Selenium Webdriver to access dynamically generated html content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_f1_race():  \n",
    "    url = 'https://www.formula1.com/'\n",
    "    browser = wd.Chrome()\n",
    "    browser.get(url)\n",
    "\n",
    "    races = []\n",
    "    countdown ={}\n",
    "\n",
    "    race_locations = browser.find_elements_by_class_name('race-name')\n",
    "    for location in race_locations:\n",
    "        if location.text.strip() is not '':\n",
    "            races.append(str(location.text.strip()))\n",
    "\n",
    "\n",
    "    race_countdown = browser.find_elements_by_class_name('countdown-text')\n",
    "    for i, time in enumerate(race_countdown, 0):\n",
    "        if i == 0:\n",
    "            countdown['Days'] = (time.text.strip())\n",
    "        if i == 1:\n",
    "            countdown['Hours'] = (time.text.strip())\n",
    "        if i == 2:\n",
    "            countdown['Minutes'] = (time.text.strip())\n",
    "\n",
    "\n",
    "    # print(races, countdown)\n",
    "    print('Location: ', races[0])\n",
    "    print('Countdown: ', countdown['Days'], 'days,', countdown['Hours'], 'hours and', countdown['Minutes'], 'minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location:  AUSTRALIA\n",
      "Countdown:  04 days, 11 hours and 25 minutes\n"
     ]
    }
   ],
   "source": [
    "next_f1_race()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 21\n",
    "nlt.corpus.words only seems to include the basic form of most english words. As you can see, as simple forms like plural do not match with any word in nltk.corpus.words.\n",
    "\n",
    "Numbers are not included either.\n",
    "\n",
    "For an even better result we could filter full-stops and other symbols that occur next to a word without whitespace.\n",
    "\n",
    "Nations does not seem to be included in nltk.corpus.words.\n",
    "\n",
    "Months does not seem to be inclded\n",
    "\n",
    "If we only wanted completely unknown words, we would need to use word normalizing and some minor regex tweaking. Also filtering with the names corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown(url):\n",
    "    tokens = times_article_paragraph(url)\n",
    "    known_words = words.words()\n",
    "    types = [w.lower() for w in tokens]\n",
    "\n",
    "\n",
    "\n",
    "    unknown_words = [w for w in types if w not in known_words]\n",
    "\n",
    "    return set(unknown_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: [For the latest updates on the Ethiopian Airlines...> \n",
      "\n",
      "no matches\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'17',\n",
       " '18',\n",
       " '189',\n",
       " '200.',\n",
       " '302',\n",
       " '737',\n",
       " '8',\n",
       " '8,',\n",
       " '8,000',\n",
       " '8’s',\n",
       " '[for',\n",
       " 'airlines',\n",
       " 'airlines,',\n",
       " 'american',\n",
       " 'attendants',\n",
       " 'boeing’s',\n",
       " 'carriers,',\n",
       " 'circumstances',\n",
       " 'co-pilot',\n",
       " 'conclusions',\n",
       " 'crash,',\n",
       " 'crew.',\n",
       " 'ethiopian',\n",
       " 'experts',\n",
       " 'fly.',\n",
       " 'here.]•',\n",
       " 'hours',\n",
       " 'including',\n",
       " 'indonesia',\n",
       " 'intensified',\n",
       " 'investigators',\n",
       " 'killed',\n",
       " 'max',\n",
       " 'monday.•',\n",
       " 'newest',\n",
       " 'notification,”',\n",
       " 'october',\n",
       " 'others',\n",
       " 'people.•',\n",
       " 'pilots',\n",
       " 'planes.',\n",
       " 'questions',\n",
       " 'recovered,',\n",
       " 'safety.•',\n",
       " 'said.',\n",
       " 'states',\n",
       " 'states,',\n",
       " 'sunday,',\n",
       " 'two-person',\n",
       " 'updates',\n",
       " 'users',\n",
       " '—',\n",
       " '“continued'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown('https://www.nytimes.com/2019/03/11/world/boeing-737-max-air-crash-ethiopia.html?action=click&module=Top%20Stories&pgtype=Homepage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sneeze():\n",
    "    seq = 'aehh '\n",
    "    fin = ''\n",
    "    for i in range(500):\n",
    "        fin += random.choice(seq)\n",
    "\n",
    "    print(fin, 'CHOOOOOOO!!!!!!!!!!!!!!!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aehaa hehahh h hhaaa aa ea hhhehha he hehe aahahaahaheh ehahahhaahh hhahhaahhha aheaaaae heeh hh eh hh   eh h e ahe eeh hh  eahe  ahhe a hahhha ehh aeha    h a   ahhh h hehaeahhe haeheaahh   h ehahhhaee aehae  ahheheeeeeeh a h h ehheaah hh  eaeehhheh ahhehhhe ahaha hhehahh hee ehhhah ehehhea e hhh  eh hh eea hh haeehe ah  ahea ehaha aeaheahahahe aaaheea h ahhh    e ae ahhhahhee heea  ahhh h hhheaha eaahe  aae eaheehhe e aeh hhhhhh ehhhh   hahhhahhehahahahaaahahaahaaaa hheee aehh aeheaaha hheaehh CHOOOOOOO!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "sneeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 30\n",
    "As we ca see form first two words, the Lancaster stemmer is quite a bit more aggressive than the Porter Stemmer. The words that come through the Porter stemmer are still quite readable and understandable while Lancaster might change the meaning completely.\n",
    "\n",
    "It is also said that the Lancaster stemmer is the least computationally intensive method. Porter is still the most usual one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.'] \n",
      "\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "raw = 'DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government.  Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.'\n",
    "tokens = word_tokenize(raw)\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "print([porter.stem(w) for w in tokens], '\\n')\n",
    "print([lancaster.stem(w) for w in tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_checker(arr):\n",
    "    lengths = []\n",
    "    for word in arr:\n",
    "        lengths.append(len(word))\n",
    "    \n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex31 = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.']\n",
    "print(len_checker(ex31))\n",
    "fgh = [len(w) for w in ex31]\n",
    "fgh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_what(): \n",
    "    silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "    bland = silly.split()\n",
    "    stringy = ''\n",
    "    for word in bland:\n",
    "        stringy += word[1]\n",
    "\n",
    "    print(stringy)\n",
    "    print(' '.join(sorted(bland)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eoldrnnnna\n",
      "an are bland formed ideas in inexpressible infuriating newly way\n"
     ]
    }
   ],
   "source": [
    "hmm_what()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 33\n",
    "a. index() will return the first index at which the sequence being. 'inexpressible'.index('re') will return 5. Same as if we looked for index('r'). If the sequence given in the index() function is not found, an error will be returned.\n",
    "\n",
    "b. Works the same as with strings\n",
    "\n",
    "c. Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a)  5\n",
      "b)  1\n",
      "c)  ['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible']\n"
     ]
    }
   ],
   "source": [
    "print('a) ', 'inexpressible'.index('re'))\n",
    "print('b) ', ['food', 'coffee', 'chair', 'table'].index('coffee'))\n",
    "sille = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "\n",
    "print('c) ', sille.split()[:sille.split().index('in')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
